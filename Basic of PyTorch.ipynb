{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch's tensor Library\n",
    "The most of PyTorch operations are running on **tensors**. A tensors is an multidimensional array. Lets have a look on basic tensor operations. But first, lets import some important PyTorch libraries:\n",
    "* **torch** - A Tensor library similar to NumPy, with strong GPU support\n",
    "* **torch.autograd** -a \"tape-based\" automatic differentiation library\n",
    "* **torch.nn** - a neural networks lirary deeply integrated with autograd\n",
    "* **torch.optim** - an optimizer package to be used with torch.nn with stantdard optimization methods such as SGD, RMPSProp,LBFGS, Adam etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f09e23548b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Tensors\n",
    "Tensors can be created form Python lists with the **torch.Tensor()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Create a torch.Tensor object from python list\n",
    "v = [1,2,3]\n",
    "print(type(v))\n",
    "v_tensor = torch.Tensor(v)\n",
    "print(type(v_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Creating a torch.Tensor object of size 2x3 form 3x3 matrix\n",
    "m1 = [1,2,3],[4,5,6]\n",
    "m1_tensor = torch.Tensor(m1)\n",
    "print(m1_tensor)\n",
    "print(type(m1_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2., 3.],\n",
       "         [2., 3., 4.],\n",
       "         [8., 5., 4.]],\n",
       "\n",
       "        [[1., 3., 2.],\n",
       "         [3., 4., 5.],\n",
       "         [9., 0., 1.]],\n",
       "\n",
       "        [[1., 4., 5.],\n",
       "         [5., 4., 6.],\n",
       "         [8., 7., 6.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 3D torch.Tensor object of size 3x3x3.\n",
    "m2 = [[[1,2,3],[2,3,4],[8,5,4]],\n",
    "      [[1,3,2],[3,4,5],[9,0,1]],\n",
    "      [[1,4,5],[5,4,6],[8,7,6]]]\n",
    "\n",
    "m2_tensor = torch.Tensor(m2)\n",
    "m2_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-2.7202,  0.5421, -1.1541],\n",
      "          [ 0.7763, -0.2582, -2.0407],\n",
      "          [-0.8016, -0.8183, -0.0480]],\n",
      "\n",
      "         [[ 0.5349,  1.1031,  1.3334],\n",
      "          [-1.4053, -0.5922, -0.2548],\n",
      "          [ 1.1517,  0.8138,  0.6532]],\n",
      "\n",
      "         [[ 0.6557, -1.4056, -1.2743],\n",
      "          [ 0.4513, -0.2280,  0.9224],\n",
      "          [ 0.8566,  0.6465,  1.2782]]],\n",
      "\n",
      "\n",
      "        [[[ 2.5501, -0.3018, -0.6703],\n",
      "          [-0.6171, -0.8334,  0.5663],\n",
      "          [ 1.0306, -0.3047,  1.6873]],\n",
      "\n",
      "         [[ 0.6851,  2.0024, -0.5469],\n",
      "          [ 1.6014, -0.3016, -0.7074],\n",
      "          [-0.1465, -0.4943, -1.1766]],\n",
      "\n",
      "         [[-2.0524,  0.1132,  1.4353],\n",
      "          [-1.1454, -1.3316,  0.2230],\n",
      "          [ 0.6463,  0.1538, -0.4452]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5503,  0.0658,  0.2225],\n",
      "          [-0.1689, -0.5455,  0.2487],\n",
      "          [ 0.1343,  0.7662,  2.2760]],\n",
      "\n",
      "         [[-1.3255, -1.0590,  0.0801],\n",
      "          [ 0.3531, -0.1207, -0.9797],\n",
      "          [-2.1126, -0.2721, -0.3510]],\n",
      "\n",
      "         [[-1.6483,  0.1536, -0.1807],\n",
      "          [-0.1086,  1.1721, -0.4372],\n",
      "          [-0.4053,  0.7086, -0.1346]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4680, -0.7952, -0.9178],\n",
      "          [-0.0673,  0.2467, -0.9392],\n",
      "          [-1.0448, -0.4698,  1.0866]],\n",
      "\n",
      "         [[-0.8892,  0.7647,  0.0526],\n",
      "          [-1.1892,  0.6751, -0.5757],\n",
      "          [ 1.0949,  1.1196,  0.1306]],\n",
      "\n",
      "         [[-0.2589, -0.4780,  0.7995],\n",
      "          [ 0.9905, -0.0730, -1.0638],\n",
      "          [-0.3050,  0.1267,  1.6921]]]])\n"
     ]
    }
   ],
   "source": [
    "# Similarly creating random data of 4D Tensor  with dimension with torh.randn\n",
    "\n",
    "m3_tensor = torch.randn((4,3,3,3))\n",
    "m3_tensor.shape\n",
    "print(m3_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional tensors\n",
    "\n",
    "Since we frequently use the tensoer of size n>3 dimensions,thefore  it is so important to understand. The best wat to think of a higher(n) dimensional object (and tensor in particular) is as a contianer which keeps a series of n-1 dimensional objects \"inside\" of it. We can \"put out\" these \"inner\" objects by indexing into higher dimesional tensor cotainer. Let's have a lock on some examples:\n",
    "* For a vector v(dim(v) = 1), indexing into it ('pulling out of it) returns its \"slice\" -a scalar s (dim(s) = 0).\n",
    "* For a matrix, indexing into it returns its \"slice\" -a (row or column) vector.\n",
    "* 3D tensor can be  seen as cube or 3D rectangular consisting of horizontally \"stacked\" matrices. So if we index into a such tensor it will give us its which is a matrix!.\n",
    "* We can't easily visualize 5D (or n-D) tensors, but the idea is actually the same if we index in to them, we will pull out an object of dimension n-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[[1, 2, 3], [2, 3, 4], [8, 5, 4]]\n",
      "tensor([[[-2.7202,  0.5421, -1.1541],\n",
      "         [ 0.7763, -0.2582, -2.0407],\n",
      "         [-0.8016, -0.8183, -0.0480]],\n",
      "\n",
      "        [[ 0.5349,  1.1031,  1.3334],\n",
      "         [-1.4053, -0.5922, -0.2548],\n",
      "         [ 1.1517,  0.8138,  0.6532]],\n",
      "\n",
      "        [[ 0.6557, -1.4056, -1.2743],\n",
      "         [ 0.4513, -0.2280,  0.9224],\n",
      "         [ 0.8566,  0.6465,  1.2782]]])\n"
     ]
    }
   ],
   "source": [
    "print(m1[0])\n",
    "print(m2[0])\n",
    "print(m3_tensor[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operation with Tensors\n",
    " As we can operate on tensors in the ways we would expect. here belo are some operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([1,2,3])\n",
    "y = torch.Tensor([4,5,6])\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32.)\n"
     ]
    }
   ],
   "source": [
    "w = torch.matmul(x,y)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8552,  0.7492, -1.7119,  0.6025, -0.7018],\n",
      "        [-1.3130,  0.1574,  2.0114,  0.1004,  0.8222]])\n"
     ]
    }
   ],
   "source": [
    "# By defualt, it concatenates along the axis with 0 (rows). It's \"stacking\" the rows.\n",
    "\n",
    "x_1 = torch.randn(2,5)\n",
    "print(x_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8985,  0.6210, -0.9679,  0.6740, -1.2828],\n",
      "        [-0.5097,  0.1464, -0.4860, -0.7529,  1.6989],\n",
      "        [ 0.4991, -2.1702,  0.5130, -1.9029,  0.8260]])\n"
     ]
    }
   ],
   "source": [
    "y_1 = torch.randn(3,5)\n",
    "print(y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_1 = torch.cat([x_1,y_1])\n",
    "z_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second arg specifies which axis to concat along, Here we select 1 (column). It's attaching the columns\n",
    "x_2 = torch.randn(2,3)\n",
    "y_2 = torch.randn(2,5)\n",
    "z_2 = torch.cat([x_2,y_2] ,1)\n",
    "z_2.shape\n",
    "\n",
    "# If Tensors are not compatible, torch will complain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Reshaping Tensors\n",
    "We can use the `.view()` method to reshape a tensor. Often we will need to reshape our data before pasing it to a neural network.\n",
    "\n",
    "Let's assume we have 64000 RGB images with size of 28x28 pixels. We can define an array to shape (64000,3, 28,28) to hold them, where 3 is number of color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2000, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(64000,3,28,28)\n",
    "# Now we add a batch dimension of size 32 then infer second dimension by placing -1:\n",
    "x_reshaped = x.view(32,-1, 3, 28,28)\n",
    "print(x_reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation Graphs and Automatic Differentiation\n",
    "\n",
    "A computation graph is a specification of parameters with which are invloved in the computation to give the ouput.\n",
    "\n",
    "The fundamental class of PyTorch `autograd.Variable` keeps of jpw it was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Variable wrap tensor objects\n",
    "x = autograd.Variable(torch.Tensor([1,2,3]),requires_grad = True)\n",
    "# You can access the data with the .data attribute\n",
    "\n",
    "print(x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = autograd.Variable(torch.Tensor([4,5,6]),requires_grad = True)\n",
    "z = x+y\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7f0a1c081970>\n"
     ]
    }
   ],
   "source": [
    "operation = z.grad_fn\n",
    "print(operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autograd.Variable knows which operation has created it. But does how that help **computes a gradient?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Lets sum up all the entries in x\n",
    "s = z.sum()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SumBackward0 object at 0x7f0a1da9e550>\n"
     ]
    }
   ],
   "source": [
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient\n",
    "So now, what is the derivative of this sum with respect to the first component of x? Remember, that  x is a tensor of 3 elements :$ x = (x_0,x_1,x_2)$\n",
    "\n",
    "In math, we want a partial derivative of $s$ with respect to $x_0:\\frac{dy}{dx_0}$\n",
    "\n",
    "Well, $s$ knows that it was created as s sum of the tensor $z$ elements $(z_0,z_1,z_2)$. $z$ knows that it was the sum $x+y$. So\n",
    "\n",
    "$$s = \\frac{z_0}{x_0 +y_0} + \\frac{z_1}{x_1 +y_1} + \\frac{z_2}{x_2 +y_2}$$\n",
    "\n",
    "And so $s$ contains enough information to determine that the derivative of $s$ with respect to $x_0$ is 1!.\n",
    "\n",
    "First we nned to run **backpropagation** and calculate gradients with respect to every variable. Note: if you run `backward` multiples times, the gradients will increment. That is because PyTroch accumulates the gradients into the **.gard property** since for many models this is very convenient. Lets now have PyTorch compute the gradient, and see we were right with our guess of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling .backward() on any variable will run backprop, starting from it.\n",
    "s.backward(retain_graph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n",
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 4., 4.])\n",
      "tensor([4., 4., 4.])\n"
     ]
    }
   ],
   "source": [
    "s.backward(retain_graph = True)\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How NOT to break the computational graph\n",
    "\n",
    "Let's create two tensor and add them up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0077, -0.0804],\n",
      "        [-0.5274,  0.0947]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((2,2))\n",
    "y = torch.randn((2,2))\n",
    "z = x + y # These are Tensor types, and backprop would not be possible\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we wrap the torch tensors in `autograd.Varible`. The `var_z` contains the information for backpropagations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7f09d7bfd460>\n"
     ]
    }
   ],
   "source": [
    "var_x = autograd.Variable(x,requires_grad = True)\n",
    "var_y = autograd.Variable(y, requires_grad  = True)\n",
    "# var_z contains enough information to compute gradients, as we saw above\n",
    "\n",
    "var_z = var_x + var_y\n",
    "print(var_z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what happens if we extract the wrapped tensor object out of var_z and re-wrap the tensor in a new autograd.Variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "var_z_data = var_z.data\n",
    "new_var_z = autograd.Variable(var_z_data)\n",
    "print(new_var_z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable chain is not existing anymore,since we have extracted only data and the whole operatiosn was lost. If we try now to compute `backward` on `new_var_z`, it will throw an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_var_z.backward(retain_graph = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA\n",
    "Check whether GPU acceleation with **CUDA** is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us tun this cell only if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Create a LongTensor and tranfers it\n",
    "    # to GPU as torch.cuda.LongTensor\n",
    "    a = torch.LongTensor(10).fill_(3).cuda()\n",
    "    print(type(a))\n",
    "    b = a.cpu()\n",
    "    # transfers it to CPU, back to\n",
    "    # being a torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
